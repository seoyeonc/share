{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c24d4d0-ac6b-436a-84cf-d0a08faec2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uqq transformers accelerate bitsandbytes  # allow efficient inference with option load_in_8bit=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1674c2-d457-435d-a740-ad6948425c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc7a639-f909-4374-b303-3419a346d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['EleutherAI/pythia-2.8B-deduped',\n",
    "               'bigscience/bloom-3b',\n",
    "               'cerebras/Cerebras-GPT-2.7B']  # bigger, may overflow RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210b9864-d7c7-42b5-bb7d-c8ad412a0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_names[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9107ce81-ed4d-4aa4-b0af-3970d2553169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making string tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print('Making string tokenizer...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e956df-c067-4b8c-adf8-c87fc36a7319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 361/361 [00:00<00:00, 2.64MB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.62MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 764kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b427f1d7-55cc-4ea9-88eb-136a292638d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model `cerebras/Cerebras-GPT-2.7B`...\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading model `{model_name}`...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c8b828e-5174-45aa-818b-9c4d18a076f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 10.7G/10.7G [01:45<00:00, 102MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # automatically store parameters on gpu, cpu or disk\n",
    "    low_cpu_mem_usage=True,  # try to limit RAM\n",
    "    load_in_8bit=True,  # load model in low precision to save memory\n",
    "    torch_dtype=torch.float16,  # load model in low precision to save memory\n",
    "    offload_state_dict=True,  # offload onto disk if needed\n",
    "    offload_folder=\"offload\",  # offload model to `offload/`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80540d7f-679a-424d-8914-b6ad44d77634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model\n"
     ]
    }
   ],
   "source": [
    "print('Finished loading model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a27fafd-0cc2-4faf-bf5c-dcc900ee0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73bbbb09-d1fb-4810-9412-bf0e50f1fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "prompt = '''The following is Python code for plotting the sine function from 0 to 2pi:'''\n",
    "# prompt = '''La ciudad más grande de América del Norte es:'''\n",
    "# prompt = '''What is a good question for a course in numerical methods for chemical engineers?'''\n",
    "# prompt = '''What is a number that can be represented exactly in decimal scientific notation, but not in binary floating point?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10b8dd7a-2642-49ca-858d-f6d8d1baed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model inference with prompt `The following is Python code for plotting the sine function from 0 to 2pi:`\n"
     ]
    }
   ],
   "source": [
    "print('Performing model inference with prompt `%s`' % prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f088a83-31f6-4f7e-9b05-96d73e4053de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output cerebras/Cerebras-GPT-2.7B:\n"
     ]
    }
   ],
   "source": [
    "print(f'Output {model_name}:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a7ace8e-4bf2-4bcb-b40b-ef0b10dd958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1063bcd8-9fd9-427a-9d1f-a7ebbc2c57de",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbd3a2a7-a1a4-467a-a62f-d86c71e12332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1adbfd43-ba17-427d-8aff-572bd6683f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8268f31c-46b3-41f0-abcd-1cd80df1a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is Python code for plotting the sine function from 0 to 2pi:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "def sine(x):\n",
      "    return np.sin(x)\n",
      "\n",
      "x = np.linspace(0, 2*np.pi, 100)\n",
      "y = np.sin(x)\n",
      "\n",
      "plt.plot(x, y)\n",
      "plt.show()\n",
      "\n",
      "The plot looks like this:\n",
      "\n",
      "The problem is that the sine function is\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5515aac7-bfe4-4fb1-995e-8e13e8d6a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 9.89s\n"
     ]
    }
   ],
   "source": [
    "print('Inference time: %.2fs' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0187400-8ffe-4fdb-8439-964bd670ec3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
